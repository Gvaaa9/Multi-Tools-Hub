<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robots.txt Generator - Multi-Tools Hub</title>
    <meta name="description" content="Generate robots.txt file for your website. Control search engine crawling and indexing with proper robots.txt directives.">
    
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <!-- Custom CSS -->
    <link href="../../assets/css/style.css" rel="stylesheet">
</head>
<body>
    <!-- Header will be loaded dynamically -->
    <div id="header"></div>

    <div class="tool-page">
        <div class="container py-5">
            <div class="row justify-content-center">
                <div class="col-lg-10">
                    <div class="tool-content">
                        <div class="tool-header">
                            <h1><i class="fas fa-robot me-3"></i>Robots.txt Generator</h1>
                            <p>Generate robots.txt file to control search engine crawling and indexing</p>
                        </div>

                        <!-- Ad Section -->
                        <div class="ad-banner text-center mb-4">
                            <div class="ad-placeholder bg-light border rounded p-3">
                                <p class="text-muted mb-0">Advertisement Space</p>
                                <small class="text-muted">Google AdSense or other ads will be placed here</small>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-6">
                                <h5>Basic Settings</h5>
                                <div class="form-group mb-3">
                                    <label for="websiteUrl" class="form-label">Website URL *</label>
                                    <input type="url" class="form-control" id="websiteUrl" placeholder="https://example.com">
                                    <div class="form-text">Enter your website's base URL</div>
                                </div>

                                <div class="form-group mb-3">
                                    <label for="sitemapUrl" class="form-label">Sitemap URL</label>
                                    <input type="url" class="form-control" id="sitemapUrl" placeholder="https://example.com/sitemap.xml">
                                    <div class="form-text">Optional: URL to your sitemap</div>
                                </div>

                                <h5 class="mt-4">Crawl Rules</h5>
                                <div class="form-group mb-3">
                                    <label for="crawlDelay" class="form-label">Crawl Delay (seconds)</label>
                                    <input type="number" class="form-control" id="crawlDelay" min="0" max="60" value="0">
                                    <div class="form-text">Delay between requests (0 = no delay)</div>
                                </div>

                                <div class="form-group mb-3">
                                    <label for="disallowPaths" class="form-label">Disallow Paths</label>
                                    <textarea class="form-control" id="disallowPaths" rows="4" placeholder="Enter one path per line:
/admin/
/private/
/temp/
*.pdf"></textarea>
                                    <div class="form-text">Paths to block from crawling</div>
                                </div>

                                <div class="form-group mb-3">
                                    <label for="allowPaths" class="form-label">Allow Paths</label>
                                    <textarea class="form-control" id="allowPaths" rows="3" placeholder="Enter one path per line:
/public/
/images/"></textarea>
                                    <div class="form-text">Paths to explicitly allow (optional)</div>
                                </div>

                                <div class="text-center">
                                    <button class="btn btn-primary btn-lg" id="generateRobotsBtn">
                                        <i class="fas fa-robot me-2"></i>Generate Robots.txt
                                    </button>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="robots-preview border rounded p-3" id="robotsPreview">
                                    <h6>Robots.txt Preview</h6>
                                    <p class="text-muted">Enter your settings and click "Generate Robots.txt" to see results</p>
                                </div>
                            </div>
                        </div>

                        <div id="result" class="result-box" style="display: none;"></div>

                        <!-- Ad Section -->
                        <div class="ad-banner text-center mt-4">
                            <div class="ad-placeholder bg-light border rounded p-3">
                                <p class="text-muted mb-0">Advertisement Space</p>
                                <small class="text-muted">Google AdSense or other ads will be placed here</small>
                            </div>
                        </div>

                        <div class="mt-5">
                            <h4>How to Use Robots.txt Generator</h4>
                            <ol>
                                <li>Enter your website's base URL</li>
                                <li>Add your sitemap URL if you have one</li>
                                <li>Set crawl delay if needed</li>
                                <li>Specify paths to disallow or allow</li>
                                <li>Click "Generate Robots.txt" to create your file</li>
                                <li>Upload the file to your website's root directory</li>
                            </ol>

                            <h5 class="mt-4">Robots.txt Directives</h5>
                            <div class="row">
                                <div class="col-md-6">
                                    <h6>Common Directives</h6>
                                    <ul>
                                        <li><strong>User-agent:</strong> Specifies which crawler</li>
                                        <li><strong>Disallow:</strong> Blocks access to paths</li>
                                        <li><strong>Allow:</strong> Explicitly allows access</li>
                                        <li><strong>Sitemap:</strong> Points to your sitemap</li>
                                    </ul>
                                </div>
                                <div class="col-md-6">
                                    <h6>Best Practices</h6>
                                    <ul>
                                        <li>Place robots.txt in root directory</li>
                                        <li>Use forward slashes for paths</li>
                                        <li>Test with Google Search Console</li>
                                        <li>Keep file under 500KB</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Footer will be loaded dynamically -->
    <div id="footer"></div>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Custom JS -->
    <script src="../../assets/js/main.js"></script>
    
    <script>
        // Initialize the tool
        document.addEventListener('DOMContentLoaded', function() {
            initializeRobotsTxtGenerator();
        });

        function initializeRobotsTxtGenerator() {
            const generateRobotsBtn = document.getElementById('generateRobotsBtn');
            const websiteUrl = document.getElementById('websiteUrl');

            // Generate robots.txt button
            generateRobotsBtn.addEventListener('click', generateRobotsTxt);

            // Real-time preview
            websiteUrl.addEventListener('input', updatePreview);
        }

        function updatePreview() {
            const url = document.getElementById('websiteUrl').value;
            const preview = document.getElementById('robotsPreview');
            
            if (url.trim()) {
                preview.innerHTML = `
                    <h6>Ready to Generate</h6>
                    <p><strong>Website:</strong> ${url}</p>
                    <p class="text-muted">Click "Generate Robots.txt" to create file</p>
                `;
            } else {
                preview.innerHTML = `
                    <h6>Robots.txt Preview</h6>
                    <p class="text-muted">Enter your settings and click "Generate Robots.txt" to see results</p>
                `;
            }
        }

        function generateRobotsTxt() {
            const websiteUrl = document.getElementById('websiteUrl').value.trim();
            const sitemapUrl = document.getElementById('sitemapUrl').value.trim();
            const crawlDelay = document.getElementById('crawlDelay').value;
            const disallowPaths = document.getElementById('disallowPaths').value.trim();
            const allowPaths = document.getElementById('allowPaths').value.trim();

            if (!websiteUrl) {
                showNotification('Please enter a website URL', 'error');
                return;
            }

            // Validate URL
            try {
                new URL(websiteUrl);
            } catch (e) {
                showNotification('Please enter a valid website URL', 'error');
                return;
            }

            const generateRobotsBtn = document.getElementById('generateRobotsBtn');
            const result = document.getElementById('result');
            
            generateRobotsBtn.disabled = true;
            generateRobotsBtn.innerHTML = '<span class="loading"></span> Generating...';

            setTimeout(() => {
                try {
                    // Generate robots.txt content
                    const robotsTxt = generateRobotsTxtContent(websiteUrl, sitemapUrl, crawlDelay, disallowPaths, allowPaths);
                    displayResults(robotsTxt);

                } catch (error) {
                    showNotification('Error generating robots.txt: ' + error.message, 'error');
                    result.innerHTML = '<p class="text-danger">Error: ' + error.message + '</p>';
                    result.style.display = 'block';
                }
                
                generateRobotsBtn.disabled = false;
                generateRobotsBtn.innerHTML = '<i class="fas fa-robot me-2"></i>Generate Robots.txt';
            }, 1000);
        }

        function generateRobotsTxtContent(websiteUrl, sitemapUrl, crawlDelay, disallowPaths, allowPaths) {
            let robotsTxt = '# Robots.txt file generated by Multi-Tools Hub\n';
            robotsTxt += `# Website: ${websiteUrl}\n`;
            robotsTxt += `# Generated: ${new Date().toISOString()}\n\n`;
            
            // Default rules for all user agents
            robotsTxt += 'User-agent: *\n';
            
            // Add crawl delay if specified
            if (crawlDelay && crawlDelay > 0) {
                robotsTxt += `Crawl-delay: ${crawlDelay}\n`;
            }
            
            // Add disallow paths
            if (disallowPaths) {
                const disallowList = disallowPaths.split('\n')
                    .map(line => line.trim())
                    .filter(line => line);
                
                disallowList.forEach(path => {
                    robotsTxt += `Disallow: ${path}\n`;
                });
            }
            
            // Add allow paths
            if (allowPaths) {
                const allowList = allowPaths.split('\n')
                    .map(line => line.trim())
                    .filter(line => line);
                
                allowList.forEach(path => {
                    robotsTxt += `Allow: ${path}\n`;
                });
            }
            
            robotsTxt += '\n';
            
            // Add sitemap if provided
            if (sitemapUrl) {
                robotsTxt += `Sitemap: ${sitemapUrl}\n`;
            } else {
                // Generate default sitemap URL
                const baseUrl = websiteUrl.replace(/\/$/, '');
                robotsTxt += `Sitemap: ${baseUrl}/sitemap.xml\n`;
            }
            
            return robotsTxt;
        }

        function displayResults(robotsTxt) {
            const result = document.getElementById('result');
            
            let resultHTML = `
                <div class="text-center">
                    <h5 class="text-success mb-3">Robots.txt Generated Successfully!</h5>
                    <p>Your robots.txt file has been created with the specified rules.</p>
                    
                    <div class="mb-3">
                        <label class="form-label">Generated Robots.txt:</label>
                        <textarea class="form-control" rows="15" readonly id="generatedRobotsTxt">${robotsTxt}</textarea>
                    </div>
                    
                    <div class="d-flex gap-2 justify-content-center">
                        <button class="btn btn-primary" onclick="copyToClipboard(document.getElementById('generatedRobotsTxt').value)">
                            <i class="fas fa-copy me-2"></i>Copy Robots.txt
                        </button>
                        <button class="btn btn-success" onclick="downloadRobotsTxt()">
                            <i class="fas fa-download me-2"></i>Download File
                        </button>
                    </div>
                    
                    <div class="mt-4">
                        <h6>Next Steps:</h6>
                        <ol class="text-start">
                            <li>Save the file as "robots.txt"</li>
                            <li>Upload it to your website's root directory</li>
                            <li>Test it with Google Search Console</li>
                            <li>Verify it's accessible at yoursite.com/robots.txt</li>
                        </ol>
                    </div>
                </div>
            `;

            result.innerHTML = resultHTML;
            result.style.display = 'block';

            // Store for download
            window.generatedRobotsTxt = robotsTxt;

            showNotification('Robots.txt generated successfully!', 'success');
        }

        function downloadRobotsTxt() {
            if (window.generatedRobotsTxt) {
                const blob = new Blob([window.generatedRobotsTxt], { type: 'text/plain' });
                const url = URL.createObjectURL(blob);
                const link = document.createElement('a');
                link.href = url;
                link.download = 'robots.txt';
                link.click();
                URL.revokeObjectURL(url);
            }
        }
    </script>
</body>
</html>
